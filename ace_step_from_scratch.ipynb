{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Imesh7/ace-step/blob/main/ace_step_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZVZ1y8dHflgA",
        "outputId": "c4c0cd81-32bf-4317-8ef2-37cdd4974249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.8.0\n",
            "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.8.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.8.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.8.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.8.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.8.0)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.8.0)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.8.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.8.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.8.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch==2.8.0)\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch==2.8.0)\n",
            "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.8.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.8.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.8.0)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.4.0 (from torch==2.8.0)\n",
            "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.25.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "  Downloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "  Downloading torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "  Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading torchaudio-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading torchaudio-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0) (3.0.3)\n",
            "Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m865.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.29.2\n",
            "    Uninstalling nvidia-nccl-cu12-2.29.2:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.29.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cpu\n",
            "    Uninstalling torch-2.9.0+cpu:\n",
            "      Successfully uninstalled torch-2.9.0+cpu\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.24.0+cpu\n",
            "    Uninstalling torchvision-0.24.0+cpu:\n",
            "      Successfully uninstalled torchvision-0.24.0+cpu\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.9.0+cpu\n",
            "    Uninstalling torchaudio-2.9.0+cpu:\n",
            "      Successfully uninstalled torchaudio-2.9.0+cpu\n",
            "Successfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.8.0 torchaudio-2.8.0 torchvision-0.23.0 triton-3.4.0\n",
            "Collecting torchcodec==0.7\n",
            "  Downloading torchcodec-0.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.4 kB)\n",
            "Downloading torchcodec-0.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchcodec\n",
            "Successfully installed torchcodec-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio soundfile numpy\n",
        "\n",
        "\n",
        "# !pip install torch==2.8.0 torchvision torchaudio soundfile numpy\n",
        "\n",
        "# !pip uninstall torchcodec\n",
        "!pip install torchcodec==0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7vRvEV0geTt",
        "outputId": "dcaecf9e-1d23-49ab-817a-cce48910963e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "4pXk5n51xbwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d80815-cc6d-4225-ad58-62a774971789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://cli.github.com/packages stable/main amd64 Packages [356 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.8 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,888 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,682 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [62.6 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,640 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [70.9 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,608 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,296 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,288 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,571 kB]\n",
            "Get:21 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.3 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,999 kB]\n",
            "Fetched 36.7 MB in 10s (3,726 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "35 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Install ffmpeg\n",
        "!apt update\n",
        "!apt install -y ffmpeg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EfSUuDrgTeiT"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from math import inf\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "# import torchcodec\n",
        "import random\n",
        "import soundfile as sf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S05Fu4g0ugMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "297ad551-5e3b-4ec8-bc34-540b470eccf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/functional/functional.py:585: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 128, 62826])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def get_mel_spectrogram(audio_path):\n",
        "  waveform, sample_rate = sf.read(audio_path)\n",
        "  waveform = torch.tensor(waveform, dtype=torch.float32).T\n",
        "  mel_transform = torchaudio.transforms.MelSpectrogram()\n",
        "  return mel_transform(waveform)\n",
        "\n",
        "\n",
        "waveform, sample_rate = sf.read(\n",
        "    '/content/drive/MyDrive/songs/Ahasa Se Oba Ananthai.wav'\n",
        ")\n",
        "\n",
        "# Convert to torch tensor\n",
        "waveform = torch.tensor(waveform, dtype=torch.float32).T  # shape: (channels, samples) or (samples,) if mono\n",
        "# print(waveform.shape)\n",
        "\n",
        "# when you do the  'MelSpectrogram' pytorch internally do the STFT(short time fourier tranform) , so need to do it again\n",
        "mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "    # sample_rate=sample_rate,\n",
        "    # n_fft=1024,\n",
        "    # hop_length=256,\n",
        "    # n_mels=80\n",
        "    )\n",
        "mel = mel_transform(waveform)\n",
        "\n",
        "print(mel.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "73Hw-kxN3ykz"
      },
      "outputs": [],
      "source": [
        "# plot the mel-spectrogram graph\n",
        "\n",
        "def render_melspectrogram(mel, title=None):\n",
        "  plt.figure(figsize=(10, 4))\n",
        "\n",
        "  # mel_to_plot = mel[0] # <-- select the index\n",
        "\n",
        "  mel = mel.squeeze(0)\n",
        "  mel_db = torch.log(mel[0])\n",
        "\n",
        "  plt.figure(figsize=(10, 4))\n",
        "  plt.imshow(\n",
        "      mel_db.numpy(),\n",
        "      origin=\"lower\",\n",
        "      aspect=\"auto\"\n",
        "  )\n",
        "  plt.colorbar(label=\"Log Energy\")\n",
        "  plt.xlabel(\"Time Frames\")\n",
        "  plt.ylabel(\"Mel Frequency Bins\")\n",
        "  plt.title(\"Mel Spectrogram\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "KREcxpYYHAbU"
      },
      "outputs": [],
      "source": [
        "# Implement 'Deep Compression AutoEncoder'(DC-AE) to achieve highly compact mel-spectrogram latent representation\n",
        "\n",
        "# https://arxiv.org/pdf/2410.10733\n",
        "\n",
        "# pls refer\n",
        "# 1. https://github.com/mit-han-lab/efficientvit\n",
        "# 2. https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "# 3. https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "\n",
        "# You may asked why not the `SD-VAE` , I think because, when on high dimention I saw drops image quality (check out\n",
        "# the above paper they figured out that issue)\n",
        "\n",
        "class DeepCompressionAutoEncoder(nn.Module):\n",
        "  def __init__(self,in_channels, *args, **kwargs) -> None:\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.encoder = Encoder(in_channels=in_channels, out_channels=32 * in_channels)\n",
        "    self.decoder = Decoder(in_channels=32 * in_channels, out_channels=in_channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x , skip_connections = self.encoder(x)\n",
        "    x = self.decoder(x, skip_connections)\n",
        "    return x\n",
        "\n",
        "\n",
        "def conv_block(in_channels, out_channels):\n",
        "  x = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(out_channels),\n",
        "  )\n",
        "\n",
        "  return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, *args, **kwargs) -> None:\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.layers = nn.ModuleList()\n",
        "    self.layers.append(conv_block(in_channels=in_channels, out_channels=out_channels))\n",
        "\n",
        "  def forward(self, x):\n",
        "    skip_connection = []\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "      skip_connection.append(x)\n",
        "    return x, skip_connection\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, *args, **kwargs) -> None:\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.layers = nn.ModuleList()\n",
        "    self.layers.append(conv_block(in_channels=in_channels, out_channels=out_channels))\n",
        "\n",
        "  def forward(self, x, skip_connections):\n",
        "    skip_connection = skip_connections[::-1]\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "      x = torch.cat([x, skip_connection.pop(0)], dim=1)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "K9hyNndGDOgp"
      },
      "outputs": [],
      "source": [
        "# Implement DiT (Diffusion Transformer)\n",
        "\n",
        "class DiffusionTransformer(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, *args, **kwargs) -> None:\n",
        "    super().__init__(*args, **kwargs)\n",
        "    # Re-check the in_channels & out , because in multi atten we did divide the value by heads\n",
        "    self.atten = MultiHeadAttention(in_channels=in_channels, out_channels=512, num_heads=8)\n",
        "    self.norm = nn.LayerNorm(512)\n",
        "\n",
        "\n",
        "\n",
        "  def sinusoidal_positional_encoding(self, max_length, dim):\n",
        "    pos = np.arange(max_length)[:,np.newaxis]\n",
        "\n",
        "    div = np.exp(np.arange(0, dim, 2) * -np.log(10000) * dim)\n",
        "    print(div)\n",
        "\n",
        "    emb = np.zeros([max_length, dim])\n",
        "    emb[:, 0::2] = np.sin(pos / div)\n",
        "    emb[:, 1::2] = np.cos(pos / div)\n",
        "    return emb\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # first flatten the latent\n",
        "    x = x.flatten(start_dim=2)\n",
        "\n",
        "    pos = self.sinusoidal_positional_encoding(x.shape[1], x.shape[2])\n",
        "    x_pos = x + pos\n",
        "    x_atten = self.atten(x_pos)\n",
        "    return x_atten\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, num_heads, *args, **kwargs) -> None:\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = in_channels\n",
        "    self.head_dim = in_channels // num_heads\n",
        "    # multiplied by 3 , beacuse we have 'q' , 'k' & 'v' seperations\n",
        "    self.qkv = nn.Linear(in_features=in_channels, out_features= 3 * out_channels)\n",
        "    self.proj = nn.Linear(in_features=out_channels, out_features=out_channels)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, samples, emb_dim = x.shape\n",
        "    # In here we have a Tensor Like [5, 12, 3*512]\n",
        "    # this is like 5 batch, 12 samples, 3 * 512 (dimentios)\n",
        "    qkv = self.qkv(x)\n",
        "\n",
        "    # Lets reshape it\n",
        "    qkv = qkv.view(batch_size, samples, 3, self.num_heads, self.head_dim)\n",
        "    qkv = qkv.permute(2 , 0 , 3 , 1 , 4)\n",
        "\n",
        "    query, key, value = qkv\n",
        "\n",
        "    mask = torch.tril(torch.ones(samples, self.head_dim))\n",
        "    x = F.scaled_dot_product_attention(query, key, value, atten_mask=mask)\n",
        "\n",
        "    proj = self.proj(x)\n",
        "    return proj\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    self.layer1 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels)\n",
        "    self.layer2 = nn.Conv1d(in_channels=in_channels, out_channels=3 * out_channels)\n",
        "    self.act = nn.SiLU()\n",
        "    self.layer3 = nn.Conv1d(in_channels=3 * in_channels, out_channels=out_channels)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    input = x\n",
        "\n",
        "    x = self.act(x)\n",
        "    x = x + input\n",
        "    return self.layer3(x)\n",
        "\n",
        "\n",
        "# Implement cross attention for the TAGS, LY\n",
        "class CrossAttention(nn.Module):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DSGbwSJPSZMu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DriveAudioDataset(Dataset):\n",
        "    def __init__(self, root_dir, n_mels=80, sample_rate=22050, target_length_sec=5.0):\n",
        "        self.root_dir = root_dir\n",
        "        self.files = [\n",
        "            f for index , f in zip(range(2), os.listdir(root_dir))\n",
        "            if f.endswith(\".wav\")\n",
        "        ]\n",
        "        self.target_samples = int(target_length_sec * sample_rate)\n",
        "\n",
        "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mels=n_mels,\n",
        "            n_fft=1024,\n",
        "            hop_length=256,\n",
        "            power=2.0\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = os.path.join(self.root_dir, self.files[idx])\n",
        "        waveform, sample_rate = sf.read(path)\n",
        "\n",
        "        if waveform.ndim == 2 and waveform.shape[1] == 2:\n",
        "          waveform = waveform.mean(axis=1)\n",
        "\n",
        "        waveform = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0)  # [1, samples]\n",
        "\n",
        "        # Randome crop\n",
        "        if waveform.shape[-1] > self.target_samples:\n",
        "            # Random start point\n",
        "            start = random.randint(0, waveform.shape[-1] - self.target_samples)\n",
        "            waveform = waveform[:, start:start + self.target_samples]\n",
        "        else:\n",
        "            # Pad if too short\n",
        "            pad_len = self.target_samples - waveform.shape[-1]\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, pad_len))\n",
        "\n",
        "        mel_spectrogram = self.mel_transform(waveform)\n",
        "        return mel_spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nn1PkSpZtiaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7800c64f-20ae-43fb-86b9-703497ec7823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: Data shape torch.Size([1, 1, 80, 431])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2100397604.py:21: UserWarning: Using a target size (torch.Size([1, 1, 80, 431])) that is different to the input size (torch.Size([1, 33, 80, 431])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = F.mse_loss(output, data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, Loss: 112745.21875\n",
            "Batch 1: Data shape torch.Size([1, 1, 80, 431])\n",
            "epoch 0, Loss: 282271.0\n",
            "Batch 0: Data shape torch.Size([1, 1, 80, 431])\n",
            "epoch 1, Loss: 268967.03125\n",
            "Batch 1: Data shape torch.Size([1, 1, 80, 431])\n",
            "epoch 1, Loss: 373488.25\n",
            "Batch 0: Data shape torch.Size([1, 1, 80, 431])\n",
            "epoch 2, Loss: 408829.40625\n",
            "Batch 1: Data shape torch.Size([1, 1, 80, 431])\n",
            "epoch 2, Loss: 100185.2890625\n",
            "Batch 0: Data shape torch.Size([1, 1, 80, 431])\n",
            "epoch 3, Loss: 10003.80078125\n",
            "Batch 1: Data shape torch.Size([1, 1, 80, 431])\n",
            "epoch 3, Loss: 112445.640625\n",
            "Batch 0: Data shape torch.Size([1, 1, 80, 431])\n",
            "epoch 4, Loss: 197203.078125\n",
            "Batch 1: Data shape torch.Size([1, 1, 80, 431])\n",
            "epoch 4, Loss: 229397.203125\n"
          ]
        }
      ],
      "source": [
        "# DC-AE\n",
        "\n",
        "#sp - Encoder is only need in Training , decoder is needed training & inference as well.\n",
        "\n",
        "# Train\n",
        "def train():\n",
        "  epochs = 5\n",
        "  dataset = DriveAudioDataset('/content/drive/MyDrive/songs/')\n",
        "  dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "  model = DeepCompressionAutoEncoder(in_channels=1)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for batch_idx, data in enumerate(dataloader):\n",
        "      print(f\"Batch {batch_idx}: Data shape {data.shape}\")\n",
        "      # render_melspectrogram(data)\n",
        "      output = model(data)\n",
        "\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss = F.mse_loss(output, data)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      print(f\"epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "\n",
        "# Inference\n",
        "\n",
        "\n",
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "y5Ixr3tCt8um"
      },
      "outputs": [],
      "source": [
        "# DiT\n",
        "\n",
        "#Train\n",
        "\n",
        "# Get the saved 'VAE' model\n",
        "# Add noise to the mel spectrogram & train\n",
        "\n",
        "class CosineSchedule():\n",
        "  def __init__(self, Timestamp):\n",
        "    s = 0.008\n",
        "    # Compute f(t) for all timesteps\n",
        "    steps = torch.arange(Timestamp + 1, dtype=torch.float32)\n",
        "    ft = torch.cos(((steps / Timestamp + s) / 1 + s) * (np.pi / 2)) ** 2\n",
        "    self.alpha_bar = ft / ft[0]\n",
        "\n",
        "    self.alphas = self.alphas_bar[1:] / self.alphas_bar[:-1]\n",
        "    self.betas = 1 - self.alphas\n",
        "    self.sqrt_alphas_bar = torch.sqrt(self.alphas_bar)\n",
        "    self.sqrt_one_minus_alphas_bar = torch.sqrt(1 - self.alphas_bar)\n",
        "\n",
        "  def noise_schedule(self, t, x0, noise=None):\n",
        "    sqrt_alpha = self.sqrt_alphas_bar[t].view(-1, 1, 1, 1)\n",
        "    sqrt_one_minus = self.sqrt_one_minus_alphas_bar[t].view(-1, 1, 1, 1)\n",
        "\n",
        "    if noise is None:\n",
        "      noise = torch.randn_like(x0)\n",
        "    return sqrt_alpha * x0 + sqrt_one_minus * noise\n",
        "\n",
        "\n",
        "def train_model():\n",
        "  epochs = 5\n",
        "  timesteps = 100\n",
        "  dataset = DriveAudioDataset('/content/drive/MyDrive/songs/')\n",
        "  dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "  model = DeepCompressionAutoEncoder(in_channels=1)\n",
        "  cosine_schedule = CosineSchedule(Timestamp=timesteps)\n",
        "  dit = DiffusionTransformer(in_channels=80)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    for batch_idx, data in enumerate(dataloader):\n",
        "      print(f\"Batch {batch_idx}: Data shape {data.shape}\")\n",
        "      latent = model(data)\n",
        "      timestep = torch.randn(timesteps)\n",
        "\n",
        "      noise_input = cosine_schedule.noise_schedule(timestep, latent)\n",
        "      latent_pred = dit(noise_input, timestep)\n",
        "      loss = F.l1_loss(latent_pred, latent)\n",
        "      loss.backward()\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "      optimizer.step()\n",
        "      print(f\"epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "\n",
        "# Inference\n",
        "\n",
        "# Remove noise\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1bm9Xq/O0M7ciCh3UTTxm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}